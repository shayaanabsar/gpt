{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('dataset.txt') as f:\n",
    "\ttext = f.read()\n",
    "\n",
    "vocabulary = sorted(list(set(text)))\n",
    "vocab_size = len(vocabulary)\n",
    "\n",
    "stoi = {v:i for i, v in enumerate(vocabulary)}\n",
    "itos = {i:v for i, v in enumerate(vocabulary)}\n",
    "\n",
    "def encode(s): return [stoi[c] for c in s]\n",
    "def decode(i): return [itos[n] for n in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = torch.tensor(encode(text))\n",
    "\n",
    "split_v = int(0.9 * len(data))\n",
    "\n",
    "train = data[:split_v]\n",
    "test = data[split_v:]\n",
    "\n",
    "print(len(train), len(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "block_size = 128\n",
    "embedding_dimensions = 64\n",
    "num_heads = 8\n",
    "head_size = embedding_dimensions // num_heads\n",
    "num_decoders = 8\n",
    "learning_rate = 0.001\n",
    "iterations = 20000\n",
    "device = 'mps'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batch(data):\n",
    "    indexes = torch.randint(0, len(data)-block_size, (batch_size,))\n",
    "    inputs = torch.stack([data[i:i+block_size] for i in indexes]).to(device)\n",
    "    outputs = torch.stack([data[i+1:i+block_size+1] for i in indexes]).to(device)\n",
    "\n",
    "    return inputs, outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.network = nn.Sequential(\n",
    "\t\t\tnn.Linear(embedding_dimensions, embedding_dimensions),\n",
    "\t\t\tnn.ReLU(),\n",
    "\t\t\tnn.Linear(embedding_dimensions, embedding_dimensions)\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\treturn self.network(data)\n",
    "\t\n",
    "test = FeedForward().forward(torch.randn(embedding_dimensions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AttentionHead(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.key = nn.Linear(embedding_dimensions, head_size, bias=False)\n",
    "\t\tself.query = nn.Linear(embedding_dimensions, head_size, bias=False)\n",
    "\t\tself.value = nn.Linear(embedding_dimensions, head_size, bias=False)\n",
    "\t\tself.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\tbatches, time, channels = data.shape\n",
    "\n",
    "\t\tkey, query, value = self.key(data), self.query(data), self.value(data)\n",
    "\t\tmat_mul_1 = query @ key.transpose(-2, -1)\n",
    "\t\tscaled = mat_mul_1 / sqrt(channels)\n",
    "\t\tmask = scaled.masked_fill(self.tril == 0, float('-inf'))\n",
    "\t\tsoftmax = F.softmax(mask, dim=-1)\n",
    "\t\tout = softmax @ value\n",
    "\n",
    "\t\treturn out\n",
    "\n",
    "test = AttentionHead().forward(torch.randn((batch_size, block_size, embedding_dimensions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.heads = nn.ModuleList([AttentionHead() for _ in range(num_heads)])\n",
    "\t\tself.projection = nn.Linear(embedding_dimensions, embedding_dimensions)\n",
    "\t\n",
    "\tdef forward(self, data):\n",
    "\t\treturn self.projection(torch.cat([head(data) for head in self.heads], dim=-1))\n",
    "\n",
    "test = MultiHeadAttention().forward(torch.randn((batch_size, block_size, embedding_dimensions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.attention = MultiHeadAttention()\n",
    "\t\tself.norm1 = nn.LayerNorm(embedding_dimensions)\n",
    "\t\tself.feed_forward = FeedForward()\n",
    "\t\tself.norm2 = nn.LayerNorm(embedding_dimensions)\n",
    "\n",
    "\tdef forward(self, data):\n",
    "\t\tdata = data + self.attention(self.norm1(data))\n",
    "\t\tdata = data + self.feed_forward(self.norm2(data))\n",
    "\n",
    "\t\treturn data\n",
    "\n",
    "test = DecoderBlock().forward(torch.randn((batch_size, block_size, embedding_dimensions)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.embedding_space     = nn.Embedding(vocab_size, embedding_dimensions)\n",
    "\t\tself.positional_encoding = nn.Embedding(block_size, embedding_dimensions)\n",
    "\t\tself.blocks = nn.Sequential(*[DecoderBlock() for _ in range(num_decoders)])\n",
    "\t\tself.norm = nn.LayerNorm(embedding_dimensions)\n",
    "\t\tself.linear = nn.Linear(embedding_dimensions, vocab_size)\n",
    "\t\t\n",
    "\tdef forward(self, data, outputs=None):\n",
    "\t\tbatches, time = data.shape\n",
    "\n",
    "\t\tdata = self.embedding_space(data) + self.positional_encoding(torch.arange(time, device=device))\n",
    "\t\tdata = self.blocks(data)\n",
    "\t\tlogits = self.linear(data)\n",
    "\n",
    "\t\tif outputs is not None:\n",
    "\t\t\tbatches, time, channels = logits.shape\n",
    "\t\t\tloss = F.cross_entropy(logits.view(batches*time, channels), outputs.view(batches*time))\n",
    "\t\telse:\n",
    "\t\t\tloss = None\n",
    "\n",
    "\t\treturn logits, loss\n",
    "\t\n",
    "\tdef generate(self, input, num_tokens):\n",
    "\t\tfor _ in range(num_tokens):\n",
    "\t\t\tinput_clipped = input[:,-block_size:]\n",
    "\t\t\tlogits,_ = self(input_clipped)\n",
    "\t\t\tlogits = logits[:, -1, :]\n",
    "\t\t\tprobabilities = F.softmax(logits, dim=-1)\n",
    "\t\t\tsampled = torch.multinomial(probabilities, 1)\n",
    "\t\t\tinput = torch.cat((input, sampled), dim=1)\n",
    "\t\treturn input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GPT()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "losses = []\n",
    "\n",
    "for i in range(iterations):\n",
    "\tx, y = get_batch(train)\n",
    "\n",
    "\tlogits, loss = model(x, y)\n",
    "\toptimizer.zero_grad(set_to_none=True)\n",
    "\tloss.backward()\n",
    "\toptimizer.step()\n",
    "\n",
    "\tif i % 100 == 0:\n",
    "\t\tx, y = get_batch(test)\n",
    "\t\tlogits, test_loss = model(x, y)\n",
    "\t\tprint(f'Train loss: {loss}. Test loss: {test_loss}')\n",
    "\tlosses.append(loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot([i for i in range(len(losses))], losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "context = torch.zeros((1, block_size), dtype=torch.long, device=device)\n",
    "output = model.generate(context, num_tokens=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(''.join(decode(output[0].tolist()[block_size:])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict, 'model')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
